{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1DPFpV2-3SEv57dqxbJsvKFqWnfKYrUxB",
      "authorship_tag": "ABX9TyPZwiyiA17vh+io7mceO+2u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristianViSa/TFM/blob/main/perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA_fJ7uL1t4u"
      },
      "source": [
        "Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cmv_LIq09Cw"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import time\n",
        "import keras.backend as K\n",
        "import math\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM, Bidirectional, Dropout, TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import auc, roc_curve, auc, mean_squared_error, mean_absolute_error\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy import concatenate\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJCVMHRUWSZs"
      },
      "source": [
        "mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
        "def calc_mape(actual, pred): \n",
        "    actual, pred = np.array(actual), np.array(pred)\n",
        "    return np.mean(np.abs((actual - pred) / actual)) * 100\n",
        "\n",
        "\n",
        "def calc_smape(A, F):\n",
        "  A, F = np.array(A), np.array(F)\n",
        "  return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCVaCO3Y1xAq"
      },
      "source": [
        "Paths y lectura de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f_UgA3opO0D"
      },
      "source": [
        "full_data_path = \"/content/drive/MyDrive/datos_web/otros/test3.csv\"\n",
        "ordered_data_path = \"/content/drive/MyDrive/datos_web/otros/cleaned_ordered.csv\"\n",
        "# ordered_data_path = \"/content/drive/MyDrive/datos_web/otros/cleaned_ordered_2.csv\"\n",
        "final_data_path = \"/content/drive/MyDrive/datos_web/otros/final_data.csv\"\n",
        "\n",
        "# test_cleaned_full_path = \"/content/drive/MyDrive/datos_web/otros/full_cleaned.csv\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWUGIlifC0wX"
      },
      "source": [
        "final_data = pd.read_csv(final_data_path)\n",
        "dates = pd.to_datetime(final_data[\"Unnamed: 0\"].values)\n",
        "final_data.drop(\"Unnamed: 0\", axis = 1, inplace=True)\n",
        "# Eliminar calles con datos erroneos/vacios\n",
        "final_data.drop(\"A290\", axis=1, inplace=True)\n",
        "final_data.drop(\"A111\", axis=1, inplace=True)\n",
        "final_data.drop(\"B54\", axis=1, inplace=True)\n",
        "final_data.drop(\"A17\", axis=1, inplace=True)\n",
        "final_data.drop(\"A180\", axis=1, inplace=True)\n",
        "final_data.drop(\"A186\", axis=1, inplace=True)\n",
        "final_data.drop(\"A222\", axis=1, inplace=True)\n",
        "final_data.drop(\"A229\", axis=1, inplace=True)\n",
        "final_data.drop(\"A250\", axis=1, inplace=True)\n",
        "final_data.drop(\"A270\", axis=1, inplace=True)\n",
        "final_data.drop(\"A302\", axis=1, inplace=True)\n",
        "final_data.drop(\"A362\", axis=1, inplace=True)\n",
        "final_data.drop(\"A365\", axis=1, inplace=True)\n",
        "final_data.drop(\"A377\", axis=1, inplace=True)\n",
        "final_data.drop(\"A95\", axis=1, inplace=True)\n",
        "final_data.drop(\"B34\", axis=1, inplace=True)\n",
        "final_data.drop(\"B46\", axis=1, inplace=True)\n",
        "final_data.drop(\"B47\", axis=1, inplace=True)\n",
        "final_data.set_index(dates, inplace=True)\n",
        "\n",
        "\n",
        "ordered_data = pd.read_csv(ordered_data_path)\n",
        "ordered_data.drop(\"Unnamed: 0\", axis = 1, inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JUDPOO3162w"
      },
      "source": [
        "Variables y grupos. Seleccionar datos del conjunto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulntu8N8DAcd"
      },
      "source": [
        "barrios = {}\n",
        "tipos = {}\n",
        "carriles = {}\n",
        "entradas = {\n",
        "    \"serreria\" : [\"A233\", \"A415\", \"A235\"],\n",
        "    \"avcid\" : [\"A143\", \"A170\", \"A71\"],\n",
        "    \"blasco\" : [\"A49\", \"A65\", \"A33\", \"A47\"]\n",
        "}\n",
        "\n",
        "print(entradas)\n",
        "salidas = {\n",
        "    \"serreria\" : [\"A74\", \"A5\", \"A51\"],\n",
        "    \"avcid\" : [\"A72\", \"A245\"],\n",
        "    \"blasco\" : [\"A234\", \"A52\"]\n",
        "}\n",
        "# print(salidas)\n",
        "\n",
        "# Barrios\n",
        "grouped = ordered_data.groupby(ordered_data.barrio)\n",
        "for name, group in grouped:\n",
        "  neighs = list(set(group.ATA))\n",
        "  barrios[name] = neighs\n",
        "# print(barrios)\n",
        "\n",
        "# Tipos de calle\n",
        "grouped = ordered_data.groupby(ordered_data.tipo)\n",
        "for name, group in grouped:\n",
        "  types = list(set(group.ATA))\n",
        "  tipos[name] = types\n",
        "# print(tipos)\n",
        "\n",
        "# Mismos carriles\n",
        "grouped = ordered_data.groupby(ordered_data.carriles)\n",
        "for name, group in grouped:\n",
        "  lanes = list(set(group.ATA))\n",
        "  carriles[name] = lanes\n",
        "# print(carriles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bshFEeA8S7n_"
      },
      "source": [
        "# Cambiar en funci√≥n de la agrupacion\n",
        "barrio = barrios['algiros']\n",
        "\n",
        "datos = final_data[barrio]\n",
        "datos = datos[\"2021-03-15 00:00:00\":\"2021-06-20 23:00:00\"]\n",
        "datos_pre = datos[\"2021-03-15 00:00:00\":\"2021-05-09 23:00:00\"]\n",
        "datos_post = datos[\"2021-05-10 00:00:00\":\"2021-06-20 23:00:00\"]\n",
        "datos = datos[\"2021-03-15 00:00:00\":\"2021-06-20 23:00:00\"]\n",
        "datos.fillna(0.01, inplace=True)\n",
        "datos[datos == 0] = 0.01\n",
        "# datos = datos.iloc[datos.index.dayofweek == 0]\n",
        "\n",
        "# Para entrenar modelo final (NO AJUSTAR PARAMETROS CON ESTE CONJUNTO)\n",
        "train = datos[\"2021-03-15 00:00:00\":\"2021-06-06 23:00:00\"]\n",
        "test = datos[\"2021-06-07 00:00:00\":\"2021-06-20 23:00:00\"]\n",
        "\n",
        "# train = datos[\"2021-03-15 00:00:00\":\"2021-05-16 23:00:00\"]\n",
        "val = datos[\"2021-05-17 00:00:00\":\"2021-06-06 23:00:00\"]\n",
        "# test = datos[\"2021-06-07 00:00:00\":\"2021-06-20 23:00:00\"]\n",
        "\n",
        "mondays_train = train.iloc[train.index.dayofweek == 0]\n",
        "tuesdays_train = train.iloc[train.index.dayofweek == 1]\n",
        "wednesdays_train = train.iloc[train.index.dayofweek == 2]\n",
        "thursdays_train = train.iloc[train.index.dayofweek == 3]\n",
        "fridays_train = train.iloc[train.index.dayofweek == 4]\n",
        "saturdays_train = train.iloc[train.index.dayofweek == 5]\n",
        "sundays_train = train.iloc[train.index.dayofweek == 6]\n",
        "\n",
        "mondays_test = test.iloc[test.index.dayofweek == 0]\n",
        "tuesdays_test = test.iloc[test.index.dayofweek == 1]\n",
        "wednesdays_test = test.iloc[test.index.dayofweek == 2]\n",
        "thursdays_test = test.iloc[test.index.dayofweek == 3]\n",
        "fridays_test = test.iloc[test.index.dayofweek == 4]\n",
        "saturdays_test = test.iloc[test.index.dayofweek == 5]\n",
        "sundays_test = test.iloc[test.index.dayofweek == 6]\n",
        "\n",
        "features = datos.shape[1]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFH3L6zECLn3"
      },
      "source": [
        "# cols = [\"A71\"]\n",
        "# datos = pd.DataFrame(final_data[cols])\n",
        "datos = final_data\n",
        "# df = datos\n",
        "datos = datos[\"2021-03-15 00:00:00\":\"2021-06-20 23:00:00\"]\n",
        "datos_pre = datos[\"2021-03-15 00:00:00\":\"2021-05-09 23:00:00\"]\n",
        "datos_post = datos[\"2021-05-10 00:00:00\":\"2021-06-20 23:00:00\"]\n",
        "datos = datos[\"2021-03-15 00:00:00\":\"2021-06-20 23:00:00\"]\n",
        "datos.fillna(0.01, inplace=True)\n",
        "datos[datos == 0] = 0.01\n",
        "# # datos = datos.iloc[datos.index.dayofweek == 0]\n",
        "\n",
        "# Cambiar en funci√≥n de la agrupacion\n",
        "barrio = barrios['algiros']\n",
        "# Para entrenar modelo final (NO AJUSTAR PARAMETROS CON ESTE CONJUNTO)\n",
        "train = datos[\"2021-03-15 00:00:00\":\"2021-06-06 23:00:00\"]\n",
        "test = datos[\"2021-06-07 00:00:00\":\"2021-06-20 23:00:00\"]\n",
        "\n",
        "# train = datos[\"2021-03-15 00:00:00\":\"2021-05-16 23:00:00\"]\n",
        "val = datos[\"2021-05-17 00:00:00\":\"2021-06-06 23:00:00\"]\n",
        "# test = datos[\"2021-06-07 00:00:00\":\"2021-06-20 23:00:00\"]\n",
        "\n",
        "# mondays_train = train.iloc[train.index.dayofweek == 0]\n",
        "# tuesdays_train = train.iloc[train.index.dayofweek == 1]\n",
        "# wednesdays_train = train.iloc[train.index.dayofweek == 2]\n",
        "# thursdays_train = train.iloc[train.index.dayofweek == 3]\n",
        "# fridays_train = train.iloc[train.index.dayofweek == 4]\n",
        "# saturdays_train = train.iloc[train.index.dayofweek == 5]\n",
        "# sundays_train = train.iloc[train.index.dayofweek == 6]\n",
        "\n",
        "# mondays_test = test.iloc[test.index.dayofweek == 0]\n",
        "# tuesdays_test = test.iloc[test.index.dayofweek == 1]\n",
        "# wednesdays_test = test.iloc[test.index.dayofweek == 2]\n",
        "# thursdays_test = test.iloc[test.index.dayofweek == 3]\n",
        "# fridays_test = test.iloc[test.index.dayofweek == 4]\n",
        "# saturdays_test = test.iloc[test.index.dayofweek == 5]\n",
        "# sundays_test = test.iloc[test.index.dayofweek == 6]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZK0XSZD4I4S"
      },
      "source": [
        "Crear la serie temporal.\n",
        "\n",
        "2 formas diferentes :\n",
        "  1 - M√©todo create_dataset\n",
        "  2 - M√©todo series_to_supervised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdBAqtg9dPCT"
      },
      "source": [
        "def create_dataset(dataset, look_back=1):\n",
        "  dataX, dataY = [], []\n",
        "  for i in range(len(dataset) - look_back):\n",
        "    a = dataset[i: (i + look_back), 0]\n",
        "    dataX.append(a)\n",
        "    dataY.append(dataset[i + look_back, 0])\n",
        "  print(len(dataY))\n",
        "  return np.array(dataX), np.array(dataY)\n",
        "  \n",
        "# timesteps = 1\n",
        "# features = datos.shape[1]\n",
        "# x_train, y_train = create_dataset(train, timesteps)\n",
        "# x_val, y_val = create_dataset(val, timesteps)\n",
        "# x_test, y_test = create_dataset(test, timesteps)\n",
        "\n",
        "\n",
        "# x_train = np.asarray(x_train).astype('float32')\n",
        "# x_train = np.reshape(x_train, (x_train.shape[0], timesteps, features))\n",
        "# y_train = np.asarray(y_train).astype('float32')\n",
        "\n",
        "# x_test = np.asarray(x_test).astype('float32')\n",
        "# x_test = np.reshape(x_test, (x_test.shape[0], timesteps, features))\n",
        "# y_test = np.asarray(y_test).astype('float32')\n",
        "\n",
        "# x_val = np.asarray(x_val).astype('float32')\n",
        "# x_val = np.reshape(x_val, (x_val.shape[0], timesteps, features))\n",
        "# y_val = np.asarray(y_val).astype('float32')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJyTWIGzSyrF"
      },
      "source": [
        "# # Convertir la serie temporal en datos supervisados\n",
        "# Serie sin tener en cuenta informacion adicional, solo la hora anterior\n",
        "def series_to_supervised(data, keys, n_in=1, n_out=1, dropnan=True):\n",
        "  n_vars = 1 if type(data) is list else data.shape[1]\n",
        "  df = pd.DataFrame(data)\n",
        "  cols, names = list(), list()\n",
        "  # input sequence (t-n, ... t-1)\n",
        "  for i in range(n_in, 0, -1):\n",
        "    cols.append(df.shift(i))\n",
        "    names += [('%s(t-%d)' % (keys[j], i)) for j in range(n_vars)]\n",
        "  # forecast sequence (t, t+1, ... t+n)\n",
        "  for i in range(0, n_out):\n",
        "    cols.append(df.shift(-i))\n",
        "    if i == 0:\n",
        "      names += [('%s(t)' % (keys[j])) for j in range(n_vars)]\n",
        "    else:\n",
        "      names += [('%s(t+%d)' % (keys[j], i)) for j in range(n_vars)]\n",
        "  # put it all together\n",
        "  agg = pd.concat(cols, axis=1)\n",
        "  agg.columns = names\n",
        "  # drop rows with NaN values\n",
        "  if dropnan:\n",
        "    agg.dropna(0, inplace=True)\n",
        "  return agg\n",
        "\n",
        "\n",
        "features = train.shape[1]\n",
        "timesteps = 2\n",
        "outputs = 1\n",
        "\n",
        "train_keys = train.keys()\n",
        "test_keys = test.keys()\n",
        "val_keys = val.keys()\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0.1,1))\n",
        "scaler = scaler.fit(train)\n",
        "train_values = scaler.transform(train)\n",
        "test_values = scaler.transform(test)\n",
        "val_values = scaler.transform(val)\n",
        "\n",
        "serie_train = series_to_supervised(train_values, train_keys, n_in=timesteps-1, n_out=2)\n",
        "serie_test = series_to_supervised(test_values, test_keys,  n_in=timesteps-1, n_out=2)\n",
        "serie_val = series_to_supervised(val_values, val_keys, n_in=timesteps-1, n_out=2)\n",
        "\n",
        "\n",
        "x_train, y_train = serie_train.values[:, :-features], serie_train.values[:, -features:]\n",
        "x_test, y_test = serie_test.values[:, :-features], serie_test.values[:, -features:]\n",
        "x_val, y_val = serie_val.values[:, :-features], serie_val.values[:, -features:]\n",
        "\n",
        "##############  Resize de los arrays ##########\n",
        "x_train = np.asarray(x_train).astype('float32')\n",
        "x_test = np.asarray(x_test).astype('float32')\n",
        "x_val = np.asarray(x_val).astype('float32')\n",
        "\n",
        "y_train = np.asarray(y_train).astype('float32')\n",
        "y_test = np.asarray(y_test).astype('float32')\n",
        "y_val = np.asarray(y_val).astype('float32')\n",
        "\n",
        "\n",
        "n_input = timesteps * features\n",
        "x_train = x_train.reshape((x_train.shape[0], n_input))\n",
        "x_test = x_test.reshape((x_test.shape[0], n_input))\n",
        "x_val = x_val.reshape((x_val.shape[0], n_input))\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjaK9QUn47NW"
      },
      "source": [
        "Creaci√≥n del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPWjMkDBXZJ4"
      },
      "source": [
        "####### Crear el modelo  ##########\n",
        "def create_model(batch, lr, epochs, show_graph):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(100, input_dim=n_input, activation='relu', name=\"Input\"))\n",
        "  model.add(Dense(40, activation='relu', name=\"Hidden\"))\n",
        "  model.add(Dense(features, activation='linear', name=\"Output\"))\n",
        "\n",
        "  model.summary()\n",
        "  if show_graph:\n",
        "    tf.keras.utils.plot_model(model, show_shapes=True)\n",
        "  return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0SYhT2OXwfo"
      },
      "source": [
        "Entrenar los modelos. Guarda la precisi√≥n obtenida con el conjunto de validaci√≥n para su posterior an√°lisis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCTg_lhUTjVx"
      },
      "source": [
        "# Entrenar los modelos\n",
        "\n",
        "# lrs = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
        "# batches = [1, 2, 4, 8, 16, 24, 32, 64, 128, 256]\n",
        "# Best configuration\n",
        "lrs = [0.0001]\n",
        "batches = [24]\n",
        "epochs = 400\n",
        "results = {}\n",
        "metrics = [\"mse\", \"mae\", \"mape\"]\n",
        "for lr in lrs:\n",
        "  for batch in batches:\n",
        "    for loss in metrics:\n",
        "      combination = str(lr) + \"-\" + str(batch) + \"-\" + str(loss)\n",
        "      model = create_model(batch, lr, epochs, True)\n",
        "      model.compile(metrics=metrics, loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
        "\n",
        "      # Path for the weights\n",
        "      name = \"/content/drive/MyDrive/datos_web/mlp/\" + combination\n",
        "      path = Path(name)      \n",
        "      if not path.exists():\n",
        "        path.mkdir(parents=True)\n",
        "      filepath = name + \"/weights.best.hdf5\"\n",
        "      es = EarlyStopping(monitor=loss, mode='min', verbose=1, patience=2)\n",
        "      checkpoint = ModelCheckpoint(filepath, monitor=loss, verbose=1, save_best_only=True, mode='min')\n",
        "      callbacks_list = [checkpoint, es]\n",
        "      history = model.fit(\n",
        "        x_train, y_train, \n",
        "        epochs=epochs, \n",
        "        batch_size=batch, \n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=callbacks_list,\n",
        "        shuffle=False,\n",
        "        verbose=1)\n",
        "      plt.title(\"MAE\")\n",
        "      plt.xlabel(\"√âpocas\")\n",
        "      plt.ylabel(\"MAE\")\n",
        "      plt.plot(history.history['mae'], label='entrenamiento')\n",
        "      plt.plot(history.history['val_mae'], label='test')\n",
        "      plt.legend()\n",
        "      # plt.show()\n",
        "      out_file = name + \"/MAE.png\"\n",
        "      plt.savefig(out_file)\n",
        "      plt.close()\n",
        "\n",
        "      plt.title(\"MSE\")\n",
        "      plt.xlabel(\"√âpocas\")\n",
        "      plt.ylabel(\"MSE\")\n",
        "      plt.plot(history.history['mse'], label='entrenamiento')\n",
        "      plt.plot(history.history['val_mse'], label='test')\n",
        "      plt.legend()\n",
        "      # plt.show()\n",
        "      out_file = name + \"/MSE.png\"\n",
        "      plt.savefig(out_file)\n",
        "      plt.close()\n",
        "      \n",
        "      plt.title(\"MAPE\")\n",
        "      plt.xlabel(\"√âpocas\")\n",
        "      plt.ylabel(\"MAPE\")\n",
        "      plt.plot(history.history['mape'], label='entrenamiento')\n",
        "      plt.plot(history.history['val_mape'], label='validacion')\n",
        "      plt.legend()\n",
        "      # plt.show()\n",
        "      out_file = name + \"/MAPE.png\"\n",
        "      plt.savefig(out_file)\n",
        "      plt.close()\n",
        "      # Write to a file the best loss obtained\n",
        "      with open(\"/content/drive/MyDrive/datos_web/otros/results.txt\", \"a\") as file_pi:\n",
        "        l = np.array(history.history['loss'])\n",
        "        l.sort()\n",
        "        out = \"Results for \" + combination + \"\\n\"\n",
        "        file_pi.write(out)\n",
        "        file_pi.write(str(l[0]))\n",
        "        file_pi.write(\"\\n ------------------------------------------------- \\n\")\n",
        "        file_pi.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN8XwStMcOTw"
      },
      "source": [
        "Hacer predicciones\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jaHwY7fcQGO"
      },
      "source": [
        "# Change the path\n",
        "path = \"/content/drive/MyDrive/datos_web/mlp/0.0001-24-mae/weights.best.hdf5\"\n",
        "model.load_weights(path)\n",
        "\n",
        "# Hacer predicciones\n",
        "train_predict = model.predict(x_train)\n",
        "test_predict = model.predict(x_test)\n",
        "val_predict = model.predict(x_val)\n",
        "\n",
        "# Inver\n",
        "train_predict_inversed = scaler.inverse_transform(train_predict)\n",
        "test_predict_inversed = scaler.inverse_transform(test_predict)  \n",
        "val_predict_inversed = scaler.inverse_transform(val_predict)  \n",
        "\n",
        "y_test_inversed = scaler.inverse_transform(y_test)  \n",
        "y_train_inversed = scaler.inverse_transform(y_train)\n",
        "y_val_inversed = scaler.inverse_transform(y_val)\n",
        "\n",
        "# train_predict_inversed = train_predict\n",
        "# test_predict_inversed = test_predict\n",
        "# val_predict_inversed = val_predict\n",
        "\n",
        "# y_test_inversed = y_test\n",
        "# y_train_inversed = y_train\n",
        "# y_val_inversed = y_val\n",
        "\n",
        "total_train_rmse = 0\n",
        "total_train_mae = 0\n",
        "total_train_mse = 0\n",
        "\n",
        "total_test_rmse = 0\n",
        "total_test_mae = 0\n",
        "total_test_mse = 0\n",
        "total_test_mape = 0\n",
        "total_test_smape = 0\n",
        "for i in range(test_predict.shape[1]):\n",
        "  train_score = math.sqrt(mean_squared_error(y_train[:, i], train_predict[:,  i]))\n",
        "  train_mse_score = mean_squared_error(y_train[:, i], train_predict[:,  i])\n",
        "  train_mae_score = mean_absolute_error(y_train[:, i], train_predict[:,  i])\n",
        "  print('Train Score: %.5f RMSE' % (train_score))\n",
        "  print('Train Score: %.5f MAE' % (train_mae_score))\n",
        "  print('Train Score: %.5f MSE' % (train_mse_score))\n",
        "  total_train_rmse += train_score\n",
        "  total_train_mae += train_mae_score\n",
        "  total_train_mse += train_mse_score\n",
        "  # # print(len(test_predict))\n",
        "  # # print(len(y_test))\n",
        "  test_score = math.sqrt(mean_squared_error(y_test[:, i], test_predict[:,  i]))\n",
        "  test_mae_score = mean_absolute_error(y_test[:, i], test_predict[:,  i])\n",
        "  test_mse_score = mean_squared_error(y_test[:, i], test_predict[:,  i])\n",
        "  test_mape_score = calc_mape(y_test[:, i], test_predict[:,  i])\n",
        "  test_smape_score = calc_smape(y_test[:, i], test_predict[:,  i])\n",
        "  print('Test Score: %.5f RMSE' % (test_score))\n",
        "  print('Test Score: %.5f MAE' % (test_mae_score))\n",
        "  print('Test Score: %.5f MSE' % (test_mse_score))\n",
        "  print('Test Score: %.5f MAPE' % (test_mape_score))\n",
        "  print('Test Score: %.5f sMAPE' % (test_smape_score))\n",
        "  total_test_rmse += test_score\n",
        "  total_test_mae += test_mae_score\n",
        "  total_test_mse += test_mse_score\n",
        "  total_test_mape += test_mape_score\n",
        "  total_test_smape += test_smape_score\n",
        "  fig, ax = plt.subplots()\n",
        "  predicted_values = test_predict_inversed[:, i]\n",
        "  real_values = y_test_inversed[:, i]\n",
        "  # print(predicted_values)\n",
        "  horas = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
        "           20, 21, 22, 23]\n",
        "  # ax.plot(horas, real_values[22:46], \"-x\",color = 'red', label = 'Real')\n",
        "  # ax.plot(horas, predicted_values[22:46], \"-x\", color = 'blue', label = 'Forecasted')\n",
        "  ax.plot(real_values, color = 'red', label = 'Reales')\n",
        "  ax.plot(predicted_values, color = 'blue', label = 'Predichos')\n",
        "  # ax.plot(real_values, color = 'red', label = 'Reales')\n",
        "  # ax.plot(predicted_values, color = 'blue', label = 'Predichos')\n",
        "  # ax.set_title(train_keys[i])\n",
        "  if len(ordered_data[ordered_data.ATA==train_keys[i]]) > 0:\n",
        "    name = ordered_data[ordered_data.ATA==train_keys[i]].desc.iloc[0]\n",
        "  else:\n",
        "    name = train_keys[i]\n",
        "  # if len(name) > 15:\n",
        "    # name = name[0:20] +\"...\"\n",
        "  ax.set_xlabel('Hour')\n",
        "  ax.set_title(name)\n",
        "  # ax.set_xticks(x)\n",
        "  # ax.set_xticks\n",
        "  # ax.set_xticklabels([v for v in test_hours[4:1800]])\n",
        "  ax.tick_params(labelsize ='medium', pad = 5, direction = 'out', rotation = 45)\n",
        "  ax.set_ylabel('Vehicles')\n",
        "  ax.set_xticks(np.arange(len(horas)))\n",
        "  ax.legend()\n",
        "  plt.xticks(rotation='vertical')\n",
        "  plt.show()\n",
        "  print(\"ATA -------> \", train_keys[i])\n",
        "  print(\"-------------------- ACABA AQUI, CALLE NUEVA -----------------------\")\n",
        "print('Total train Score: %.4f RMSE' % (total_train_rmse))\n",
        "print('Total train Score: %.4f MAE' % (total_train_mae))\n",
        "print('Total train Score: %.4f MSE' % (total_train_mse))\n",
        "\n",
        "print('Total test Score: %.4f RMSE' % (total_test_rmse))\n",
        "print('Total test Score: %.4f MAE' % (total_test_mae))\n",
        "print('Total test Score: %.4f MSE' % (total_test_mse))\n",
        "print('Total test Score: %.4f MAPE' % (total_test_mape))\n",
        "print('Total test Score: %.4f sMAPE' % (total_test_smape))\n",
        "\n",
        "\n",
        "print('AVG train Score: %.4f RMSE' % (total_train_rmse / features))\n",
        "print('AVG train Score: %.4f MAE' % (total_train_mae / features))\n",
        "print('AVG train Score: %.4f MSE' % (total_train_mse / features))\n",
        "\n",
        "\n",
        "print('AVG test Score: %.4f RMSE' % (total_test_rmse / features))\n",
        "print('AVG test Score: %.4f MAE' % (total_test_mae / features))\n",
        "print('AVG test Score: %.4f MSE' % (total_test_mse / features))\n",
        "print('AVG test Score: %.4f MAPE' % (total_test_mape / features))\n",
        "print('AVG test Score: %.4f sMAPE' % (total_test_smape / features))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzj_yCQny0Jm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79bf3f94-47b6-43a9-df98-70f081e49189"
      },
      "source": [
        "train_mse_score = mean_squared_error(y_train, train_predict)\n",
        "print(\"TRAIN MSE\", train_mse_score)\n",
        "train_mae_score = mean_absolute_error(y_train, train_predict)\n",
        "print(\"TRAIN MAE\", train_mae_score)\n",
        "train_rmse_score = math.sqrt(mean_squared_error(y_train, train_predict))\n",
        "print(\"TRAIN RMSE\", train_rmse_score)\n",
        "\n",
        "val_mse_score = mean_squared_error(y_val, val_predict)\n",
        "print(\"VAL MSE\", val_mse_score)\n",
        "val_mae_score = mean_absolute_error(y_val, val_predict)\n",
        "print(\"VAL MAE\", val_mae_score)\n",
        "val_rmse_score = math.sqrt(mean_squared_error(y_val, val_predict))\n",
        "print(\"VAL RMSE\", val_rmse_score)\n",
        "\n",
        "train_mse_score = mean_squared_error(y_test, test_predict)\n",
        "print(\"TEST MSE\", test_mse_score)\n",
        "train_mae_score = mean_absolute_error(y_test, test_predict)\n",
        "print(\"TEST MAE\", test_mae_score)\n",
        "test_rmse_score = math.sqrt(mean_squared_error(y_test, test_predict))\n",
        "print(\"TEST RMSE\", test_rmse_score)\n",
        "test_mape_score = mape(y_test, test_predict)\n",
        "print(\"TEST MAPE\", test_mape_score)\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN MSE 0.009449562\n",
            "TRAIN MAE 0.062286764\n",
            "TRAIN RMSE 0.09720885791867295\n",
            "VAL MSE 0.0106085185\n",
            "VAL MAE 0.06663623\n",
            "VAL RMSE 0.10299766257617567\n",
            "TEST MSE 0.009413776\n",
            "TEST MAE 0.06800731\n",
            "TEST RMSE 0.11872667958883296\n",
            "TEST MAPE tf.Tensor(21.684553, shape=(), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}